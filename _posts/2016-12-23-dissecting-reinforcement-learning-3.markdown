---
layout: post
title:  "Dissecting Reinforcement Learning-Part.2"
date:   2016-12-23 19:00:00 +0000
description: Explaining the basic ideas behind reinforcement learning. In particular, Markov Decision Process, Bellman equation, Value iteration and Policy Iteration algorithms, policy iteration through linear algebra methods. It includes full working code written in Python.
author: Massimiliano Patacchiola
comments: false
published: false
---

[MC problem and possible ways to solve them with TD][6.2 pag. 138]
[Bootstrapping]
[What TD Learning is]
[TD(0) the simplest form of TD learning]
[Eligibility traces and TD(lamda)]
[TD generally converge faster the MC on stochastic task][Example 6.2 pag. 139]
[Batch Learning]
[SARSA (on-policy) TD control]
[On-Policy and Off-Policy]
[Q-Learning]



Conclusions
-----------



Resources
----------

The [dissecting-reinforcement-learning](https://github.com/mpatacchiola/dissecting-reinforcement-learning) repository.

The [setosa blog](http://setosa.io/) containing a good-looking simulator for Markov chains.

Official [github repository](https://github.com/aimacode) for the book *"Artificial Intelligence: a Modern Approach"*.

References
------------

Bellman, R. (1957). A Markovian decision process (No. P-1066). RAND CORP SANTA MONICA CA.

Russell, S. J., Norvig, P., Canny, J. F., Malik, J. M., & Edwards, D. D. (2003). Artificial intelligence: a modern approach (Vol. 2). Upper Saddle River: Prentice hall.



